{
    "课程名称": "强化学习基础",
    "章节": [
        {
            "章节名": "强化学习问题场景",
            "知识点": [
                {
                    "名称": "格子世界随机性",
                    "内容": "在格子世界中，智能体采取动作时存在随机性。具体而言，智能体在某一位置执行动作时，仅有80%的概率会移动到目标方向，而剩余20%的概率会被平摊到垂直于目标方向的两个方向上。例如，在3×4的格子世界中，智能体在(3,1)位置选择向上移动时，有80%的概率到达(3,2)，10%的概率到达(2,1)，另外10%的概率到达(4,1)。如果目标方向存在障碍，则智能体会停留在原地。此外，当智能体尝试向墙壁移动时，它将保持在当前位置，如从(3,2)向左移动时，由于墙壁阻挡，智能体会留在(3,2)。"
                },
                {
                    "名称": "奖励类型",
                    "内容": "强化学习中的奖励分为即时奖励和常识奖励两种类型。即时奖励是在每一步动作执行后获得的奖励值，通常为负值，表示生存成本。例如，在格子世界中，每步可能会扣减0.1至0.3分作为生存代价。常识奖励则是在特定状态（如游戏结束）下获得的较大奖励值，用于指示目标状态。例如，在(4,3)位置获得+1奖励，在(4,2)位置获得-1奖励。这些奖励机制共同决定了智能体的行为策略。"
                },
                {
                    "名称": "策略定义",
                    "内容": "策略是指在每个状态下推荐的动作。对于格子世界问题，由于每一步动作存在不确定性，传统的动作序列解法并不适用，因此需要使用策略来解决。策略可以通过算法生成，表现为每个状态下推荐的动作方向，例如右上角的箭头表示在该位置推荐的动作。具体而言，策略是一个映射函数，输入为当前状态，输出为推荐动作的概率分布或确定性动作。"
                }
            ]
        },
        {
            "章节名": "序列决策问题",
            "知识点": [
                {
                    "名称": "状态与动作",
                    "内容": "智能体在不同时间片采取动作，环境根据动作反馈奖励并改变状态。在格子世界中，智能体每一步动作会导致环境状态变化，并伴随即时奖励或常识奖励。例如，智能体在某状态采取向右动作，环境可能返回新的状态和奖励值。更一般化来看，智能体在每个时间片采取动作，环境根据动作给出反馈，反馈包括奖励和新状态。"
                },
                {
                    "名称": "轨迹序列",
                    "内容": "轨迹序列包含状态、动作、奖励及状态转移信息。通过智能体与环境的连续交互，可以生成轨迹序列。例如，从初始状态s1开始，智能体采取动作a1，环境返回奖励r1并转移到状态s2，接着继续交互直至游戏结束。轨迹序列的形式为(s1, a1, r1, s2, a2, r2, ...)。轨迹序列不仅描述了智能体的行为路径，还记录了每一步的状态变化和奖励反馈。"
                }
            ]
        },
        {
            "章节名": "马尔可夫决策过程（MDP）",
            "知识点": [
                {
                    "名称": "马尔可夫过程",
                    "内容": "马尔可夫过程由状态集合和状态转移模型组成。状态集合是有限的，状态转移模型描述了当前状态下转移到下一状态的概率分布。例如，在火星车例子中，状态集合为{s1, s2, ..., s7}，状态转移矩阵描述了从一个状态转移到另一个状态的概率分布。具体而言，火星车在走廊空间中移动，遵循一定的概率规则，如在s1停留的概率为60%，向右移动到s2的概率为40%。"
                },
                {
                    "名称": "马尔可夫奖励过程",
                    "内容": "马尔可夫奖励过程在马尔可夫过程基础上引入奖励函数和衰减因子。奖励函数描述了在某一状态下智能体期望获得的奖励值，衰减因子用于计算累积回报时对未来的奖励进行折现。例如，在火星车例子中，奖励函数为七维向量，分别对应每个状态的奖励值，如s1奖励为+1，s7奖励为+10，其余状态奖励为0。衰减因子γ用于调整未来奖励的重要性，确保近期奖励比远期奖励更具影响力。"
                },
                {
                    "名称": "累积回报计算",
                    "内容": "累积回报通过加权和计算，考虑奖励值和时间优先级。公式为Gt = rt + γrt+1 + γ²rt+2 + ...，其中γ为衰减因子。例如，在火星车从s4到s7的轨迹中，累积回报为0 + 1/2 × 0 + 1/4 × 0 + 1/8 × 10 = 1.25。累积回报的计算依赖于奖励序列和衰减因子，能够量化智能体在某一轨迹上的总收益。"
                },
                {
                    "名称": "赛车例子",
                    "内容": "赛车问题被建模为马尔可夫决策过程，状态为空间中的引擎温度（冷、热、爆缸），动作为加速或减速。状态转移模型描述了不同状态下采取不同动作后的状态转移概率，奖励模型描述了不同状态下采取不同动作后的奖励值。例如，在引擎冷的状态下减速会以100%概率回到冷状态并获得+1奖励，加速则以50%概率保持冷状态并获得+2奖励，另50%概率转为热状态并获得+2奖励。若在热状态下加速，将直接导致爆缸，游戏结束。"
                },
                {
                    "名称": "格子世界MDP",
                    "内容": "格子世界被建模为马尔可夫决策过程，状态为格子位置，动作为上下左右。状态转移模型描述了智能体在某一位置采取某一动作后的状态转移概率，奖励模型描述了不同状态下采取不同动作后的奖励值。例如，在(3,1)位置向上移动时，有80%概率到达(3,2)，10%概率到达(2,1)，10%概率到达(4,1)，同时伴随奖励值。此外，格子世界中部分位置不可达（如墙壁），且存在终止状态（如(4,2)和(4,3)）。"
                },
                {
                    "名称": "奖励与策略关系",
                    "内容": "不同的奖励设定会对策略选择产生影响。例如，在格子世界中，单步代价较小时，智能体会选择绕道以避免风险；单步代价较大时，智能体会选择直接路径以减少消耗；当单步代价极大时，智能体会选择尽快结束游戏，即使结束状态为负奖励。具体而言，当单步代价为-0.01时，智能体倾向于绕道以规避风险；当单步代价增加至-0.03或更高时，智能体更倾向于选择直接路径；当单步代价达到极端值（如-2.1）时，智能体会优先选择尽快结束游戏，即使结束状态为负奖励。"
                }
            ]
        }
    ]
}