1
--> Speaker: 教师
00:00:00,600 --> 00:00:07,720
好，那么在接下来的章节的时间里，我们会学习强化学习的相关内容。

2
--> Speaker: 教师
00:00:08,260 --> 00:00:16,690
那么在讲强化学习之前，咱们先来看看强化学习他所要面对的一些问题，或者说实际的场景是怎么样的。

3
--> Speaker: 教师
00:00:16,690 --> 00:00:59,660
那么咱们先看一个例子，这个例子是一个格子世界的例子，那么咱们的智能体它在一个3×4的这样的1个格子空间里面去活动，他可以采取4个动作，上下左右，然后我们可以看到之前咱们碰到的一些问题，每个动作的执行结果它都是确定的，比如说如果智能体它在31的位置，那么它采取向上的动作，它就百%的32这样的一个位置，但现在咱们这个问题增加了一些随机性，智能体采取的动作不能够百%的被执行。

4
--> Speaker: 教师
00:00:59,660 --> 00:01:30,550
怎么说他有80%的可能性去到自己想去的方位，还有20%平摊到了垂直于他想去的方向的两个方向，还是刚才那个位置智能体在31，那么它采取了向上的动作，那么有80%的可能性，它会出现在32这个位置，那么有10%的可能性，它会出现在21这个位置有剩余10%的可能性，它会在41这个位置。

5
--> Speaker: 教师
00:01:30,550 --> 00:02:00,430
如果说他行动的方向有一堵墙，比如说我们现在看到22它是一堵墙，那么它就会停留在原地，也就是说如果智能体它在32的位置采取了向左的动作，而且恰巧有80%的概率它执行了向左的动作，那么因为碰到了墙它会停留在原地，那么这是这个格子世界当中智能体采取行动有随机性的一个表现。

6
--> Speaker: 教师
00:02:00,430 --> 00:02:36,060
好了，那么我们接着再来看，智能体在格子世界当中进行运动，这个过程当中它会收集到不同类型的奖励，那么在当前咱们碰到的问题里，奖励可以大致上理解为有两种类型，第一种类型可以称之为及时奖励，也就是每采取一个动作都可以获得的奖励的分值，那这个分值有可能是负的，如果说是负的，它就变成是在这个世界当中的一个生存的代价。

7
--> Speaker: 教师
00:02:36,060 --> 00:02:42,380
每采取一个动作就会扣0.1分，0.2分0.3分。

8
--> Speaker: 教师
00:02:43,390 --> 00:03:04,600
那么大家可能会发现我们在格子的42这个位置跟43这个位置分别有一个比较大的数值，那么43这个位置是正一，42这个位置是-1，那么这个就表示如果说我到达了这两个位置，游戏就结束了，并且可以获得相应分值的奖励。

9
--> Speaker: 教师
00:03:04,600 --> 00:03:13,700
那么这种类型的奖励咱们可以把它理解成是一种常识的奖励，游戏结束之后的统一奖励。

10
--> Speaker: 教师
00:03:13,700 --> 00:03:34,390
智能体在格子世界当中运动会收集到不同的奖励，有一种是即时奖励是短期的，每执行一步就可以获得一个，还有一个是在游戏结束的时候获得的一个常识奖励，那么这样的一个奖励往往是在游戏设定的，一开始我们就给定了。

11
--> Speaker: 教师
00:03:34,390 --> 00:03:36,260
好，说到这个游戏智能体会在格子世界当中去运动，收集不同的奖励，我们会希望在游戏结束的时候，智能体所获得的奖励的价格要越大越好。

12
--> Speaker: 教师
00:03:36,270 --> 00:03:54,460
接着对于这样的一个问题，我们就需要去对它进行求解了。

13
--> Speaker: 教师
00:03:54,910 --> 00:04:17,470
如果说是咱们之前碰到的一些一般化的搜索问题，算法给出来的结果是一个执行的动作序列，比如说智能体如果在31这个位置，那么我们的算法会给他推荐一个动作序列，这个动作序列是向上，然后向右就可以到达正一这样的一个位置来结束游戏。

14
--> Speaker: 教师
00:04:17,470 --> 00:04:46,570
那么大家想想，如果说我们推荐智能体系，采取这样的一个动作序列，在当前这个问题的设定下是不是行得通很显然因为每一步的执行它都是有不确定性的，所以我告诉智能体说你需要向上向右，结果它出现在了一一原始的位置上，所以动作序列对于这个问题来讲，它不是一个有效的解。

15
--> Speaker: 教师
00:04:46,570 --> 00:04:58,070
所以对于格子世界这个问题，我们想要想出来这个答案是策略，那么这个策略它给定的是我在每一个状态下，你应该采取什么样的动作。

16
--> Speaker: 教师
00:04:58,830 --> 00:05:16,880
那么在这个课件的右上角，咱们可以看到每个格子上都有一个箭头，那么这个箭头就表示在这个位置上，咱们的算法推荐你执行的动作，给每个状态找到一个推荐的执行动作，就是这个问题的解也就是称之为策略。

17
--> Speaker: 教师
00:05:16,890 --> 00:05:35,390
好了，那么看了格子世界，我们会知道在格子世界里，智能体它会跟环境发生很多轮的交互，那么发生很多轮的交互的这样一系列的问题，我们就称之为序列决策问题。

18
--> Speaker: 教师
00:05:36,290 --> 00:05:43,340
更一般化的来看待在设定当中有智能体，也就是我们算法的执行对象。

19
--> Speaker: 教师
00:05:43,340 --> 00:06:04,700
另外一个是外部的世界会对我们智能体的执行对象给出一些反馈，那么在不同的时间片智能体都会对外部世界采取一个动作，然后外部世界会根据这个动作给出一些反馈，外部世界给的反馈，我们也称之为是奖励。

20
--> Speaker: 教师
00:06:04,700 --> 00:06:30,280
那么在格子世界里外部世界给出来的反馈它就有两种类型，一种是我们生存的代价，每走一步就告诉你，你的生命值又降低了一个固定的分数，然后在游戏结束的时候会给一个比较常识的奖励，比如说你找到了宝藏给你挣一分，或者是你去到了一个火焰山，然后扣一分，大侠重新来过，那么这是环境给出来的奖励。

21
--> Speaker: 教师
00:06:30,280 --> 00:06:38,800
同时因为你采取的动作对于整个环境产生了一些影响，使得我们环境的状态发生了变化。

22
--> Speaker: 教师
00:06:38,800 --> 00:06:43,580
所以环境会告诉你说当前的状态是什么样子的。

23
--> Speaker: 教师
00:06:45,350 --> 00:07:18,850
那么在不断的智能体跟环境的交互的过程当中，咱们就可以产生一个轨迹的序列，那么这个轨迹的序列里面就包含了状态，包含了智能体的动作，包含了环境给予的奖励，以及采取动作之后整个环境发生的状态的变化，所以这个轨迹就可以是状态s一，在s一状态下采取的动作a一，环境给予的奖励r一状态转移去的一个状态s二。

24
--> Speaker: 教师
00:07:18,850 --> 00:08:00,650
那么接着在新的状态下，智能体就采取新的动作，会有a2r2、s3、a三r三，一直往后，那么在整个设定下，我们就有几个基本的元素，智能体会采取动作，环境会有一个状态的转移的告知，以及会给一个你获得的奖励的告知，也就是r那么刚才咱们讲了序列决策问题，那么序列决策问题它是指我们智能体跟环境之间有连续的交互过程，那么我们想要在这个过程当中为智能体应对不同的状态找到一个更好的动作。

25
--> Speaker: 教师
00:08:01,750 --> 00:08:07,780
那么为了解决这个问题，咱们就需要引入一个新的模型，那么这个模型就叫马尔可夫决策过程。

26
--> Speaker: 教师
00:08:07,790 --> 00:08:08,300
在讲马尔可夫决策过程之前，咱们先看一看它的最简化版本，也就是马尔可夫过程。

27
--> Speaker: 教师
00:08:09,680 --> 00:08:40,040
那么马尔可夫过程里面有两个最基本的元素，它主要是用来建模一个时间序列上面的一些基本的问题，那么这两个元素包括了一个状态集合，一般这样的一个状态集合我们会认为它是有限的，也就是说里面包含的状态的个数是可计数的。

28
--> Speaker: 教师
00:08:40,040 --> 00:09:17,930
那么第二个元素是状态转移的模型，它表示的是我在给定时间t下的某一个状态的时候，在时间片t加一下这个状态取值的一个概率分布好了，那么在更一般化的情况下，假设我们有一个状态的转移序列，从第一个时间片开始，s一s二一直到ST加一，那么对于ST加一来说，它的一个取值概率分布应该是要受到从第一个时间片到第t个时间片的所有状态的一个影响的。

29
--> Speaker: 教师
00:09:17,930 --> 00:09:30,190
那么如果要去建模这样的实际情况的话，条件概率它就会变得非常的复杂，因为在条件区域我们就要考虑从第一个时间片到第t个时间片的所有状态。

30
--> Speaker: 教师
00:09:30,770 --> 00:09:35,720
那么为了简化这样的一个模型，我们就引入了马尔可夫性。

31
--> Speaker: 教师
00:09:36,180 --> 00:09:54,630
那么马尔可夫的性质它表示的意思是对于ST加一的一个状态分布的建模，如果我给定了它前一个时刻也就是t时刻下的状态，那么它就独立于从第一个时间片到t减一个时间片的所有的状态。

32
--> Speaker: 教师
00:09:55,900 --> 00:10:07,910
在这种情况下，我在对于ST加一的一个条件概率的建模，就只需要去考虑 ST的一个状态的情况了。

33
--> Speaker: 教师
00:10:08,910 --> 00:10:22,620
那么做了这样的一个考虑之后，马尔可夫过程当中的转移模型，它就是在给定前一个时刻的状态分布，对于当前时刻状态分布的一个条件概率。

34
--> Speaker: 教师
00:10:22,620 --> 00:10:46,870
假设我们有 n个状态，那么这个状态转移模型它就是一个n乘n的矩阵，那么在这样的一个矩阵里，第一行表示如果说前一个状态是s一的话是第一个状态，那么在当前时刻下，我们的这个状态是s一s21直到sn的概率分布了。

35
--> Speaker: 教师
00:10:46,870 --> 00:11:16,660
好，那么咱们用一个火星车这样的例子来看看马可夫模型，火星车这个例子是指我们在一个走廊式的空间，也就是左右相邻的这样的一个空间里面，然后有一辆火星车它会来进行左右向的移动，那么总共有7个位置，每个位置都会对应着一个状态，所以这个问题我们就可以建模成有7个状态的马可夫过程。

36
--> Speaker: 教师
00:11:16,660 --> 00:11:25,840
那么这7个状态分别是ses2s3s4s5s6ST分别对应着走廊里面的7个不同的位置。

37
--> Speaker: 教师
00:11:27,090 --> 00:11:36,630
那么火星车它有遵循着一定的概率来在这个空间里面进行左右的移动。

38
--> Speaker: 教师
00:11:37,180 --> 00:11:46,600
比如说如果它在状态s一这个位置的话，它有60%会停留在原地，那么有40%的概率会向右移动到s二。

39
--> Speaker: 教师
00:11:46,930 --> 00:12:00,600
那么如果它当前在s二这个状态，它有20%的概率会停留在原地，有40%的概率会向左边移动到s一，有40%的概率会向右边移动到s三。

40
--> Speaker: 教师
00:12:00,610 --> 00:12:15,670
那么以此类推，对这样的一个问题，咱们如果把它用马尔可夫过程来进行描绘的话，我们就知道有7个状态，s1到s7，这是第一个元素状态空间。

41
--> Speaker: 教师
00:12:15,670 --> 00:12:32,590
那么第二个元素是转移模型，那么转移模型是7×7的矩阵，分别表示我在当前时间片下，如果是某一个状态的话，那么在下一个时间片我会转移到另外一个状态的概率分布。

42
--> Speaker: 教师
00:12:32,590 --> 00:12:51,540
那么在这个矩阵里面我们可以看到第一行就表示如果我当前在状态s一，那么下一个状态会去往的不同状态的概率分布，我有60%还是在s一，有40%去到s二，那么剩下的去到s34567的概率分别是0。

43
--> Speaker: 教师
00:12:51,540 --> 00:13:03,480
那么第二行表示的是如果当前这个状态是s二，那么下一个状态是s一的概率是40%，s二的概率是20%，s三的概率是40%以此类推。

44
--> Speaker: 教师
00:13:03,480 --> 00:13:18,600
那么因为火星车在走廊式的空间里面，它只能够停留在原地，或者是向左移动一格，或者是向右移动一格，所以这个状态转移矩阵它的数值也都集中在对角线附近。

45
--> Speaker: 教师
00:13:18,600 --> 00:13:29,210
对马尔可夫过程来讲，我们会关心它产生的轨迹，那么马尔可夫过程所产生的轨迹，它包含的元素就是状态。

46
--> Speaker: 教师
00:13:29,600 --> 00:13:52,420
那么假设咱们马可夫过程是从状态s4开始的，它可能产生的轨迹 s4、s5、s6、s7、s7、s7，那么每条轨迹大家可以知道，我们可以给它算一个产生的概率，需要做的就是把中间每个转移所对应的概率的值来进行连乘，就是这个轨迹所产生的概率了。

47
--> Speaker: 教师
00:13:52,420 --> 00:14:27,610
那么另外一个例子是s4、s4、s5、s3、s4、s5，有同学可能发现了s5到s三这个过程相当于是跨越了s四而达到的，那么这显然是不行的，因为火星车它在这个空间里面只能够停留在原地，或者是左右各移动一格，或者是左右移动向左，或者是向左或者向右移动一格，那么我们不能够完成从s5到s3的这样的一个单个时间片的状态转移。

48
--> Speaker: 教师
00:14:27,610 --> 00:14:33,790
所以第二个例子是不可能由当前的火星车的马可夫过程所产生的。

49
--> Speaker: 教师
00:14:34,560 --> 00:15:01,200
好，那么剩余的两个例子，445455是可行的，433322也是可行的，马尔可夫过程是非常简单的，那么我们也知道它离咱们目标的马尔可夫决策过程还是有一段距离的，那么咱们再往前迈一步，在马尔可夫过程上加上奖励这样的一个因素，那么就形成了马尔可夫奖励过程。

50
--> Speaker: 教师
00:15:01,200 --> 00:15:30,230
马尔可夫奖励过程里面有最基本的4个元素，那么状态空间跟转移模型都是跟马可夫过程是一致的，在马可夫奖励过程里面引入了奖励这样的一个新的概念，它是指我在经过某一个状态的时候，智能体能够收获到的期望奖励是多少，所以它所对应的是每一个状态所对应的一个值。

51
--> Speaker: 教师
00:15:30,230 --> 00:15:39,920
那么另外一个元素是伽马，也就是衰减因子，那么对于衰减因子的使用，我们会在后面慢慢的提到。

52
--> Speaker: 教师
00:15:39,920 --> 00:15:55,990
好了，对于马尔可夫奖励过程，我们同样有一个概率转移矩阵，它表示在当前这个状态下，经过一个时间片的执行，去到下一个状态的干分布是多少。

53
--> Speaker: 教师
00:15:57,040 --> 00:16:11,230
那么我们还有一个奖励矩阵，它的维度跟状态的个数是一样的，每一个位置都表示在经过某一个状态的时候能够获得的期望奖励是多少？

54
--> Speaker: 教师
00:16:11,240 --> 00:16:11,250
好。

55
--> Speaker: 教师
00:16:11,250 --> 00:16:19,910
我们具体来看一下马尔可夫奖励过程，在火星车这样的一个例子上，它能给出来的是什么？

56
--> Speaker: 教师
00:16:22,200 --> 00:16:35,720
火星车在一个走廊式的空间里面移动，它有可能会向左，有可能会向右，那么这个左右移动会依循着一个状态转移的模型，这是马可夫过程给我们的。

57
--> Speaker: 教师
00:16:35,720 --> 00:16:58,230
那么马尔可夫奖励过程给我们添加了一个奖励的信息，每个状态都对应着一个奖励值，它是一个七维的向量，那么每一个维度都对应着其中的一个在状态，s一我们可以获得正一的奖励，状态s7我们可以获得正10的奖励。

58
--> Speaker: 教师
00:16:58,240 --> 00:17:03,870
那么23456所对应的奖励分别就是0。

59
--> Speaker: 教师
00:17:03,870 --> 00:17:20,230
好，对于马可夫奖励过程，我们同样可以去为他产生一些最基本的轨迹，那么这个轨迹里就包含了两种元素，第一个元素是状态，第二个元素是这个状态所对应的奖励值。

60
--> Speaker: 教师
00:17:20,230 --> 00:17:54,750
那么同样从状态s4出发，我们从s4出发获得的奖励是0，然后接着去到s5，获得的奖励是0，去到s6获得的奖励是0，去到s7获得的奖励是正10，那么这是一个轨迹，当然这个轨迹还会继续往下走，随着这个状态转移的不断发生好了，那么大家应该还记得，对于马可夫奖励过程来讲，还有一个很重要的元素，也就是衰减因子、衰减因子伽马。

61
--> Speaker: 教师
00:17:54,750 --> 00:17:57,020
衰减因子伽马它存在的作用是什么？

62
--> Speaker: 教师
00:17:57,030 --> 00:18:04,930
它是辅助我们去计算一个奖励序列它所形成的累计回报。

63
--> Speaker: 教师
00:18:05,700 --> 00:18:16,730
对于马尔可夫决策过程来说，我们关心的是智能体从某个状态出发到这个游戏结束的时候，它所获得的奖励的累积回报是多少？

64
--> Speaker: 教师
00:18:16,740 --> 00:18:30,850
那么奖励的累积回报它其实对应的是一个奖励序列，就好像刚才火星车的例子，我们从s4到s7这个状态我获得了4个奖励，分别是000证实。

65
--> Speaker: 教师
00:18:30,850 --> 00:18:36,200
好对于这样的一个奖励的序列，我们希望给他计算一个累计回报。

66
--> Speaker: 教师
00:18:36,720 --> 00:18:52,540
那么在计算累计回报的时候，咱们会遵循两个最基本的原则，那么这个原则第一个是我所获得的奖励的值越大越好，我所获得的奖励越早拿到越好。

67
--> Speaker: 教师
00:18:53,540 --> 00:19:09,430
好了，为了去实践这两个原则，那么我们对于累积回报的一个计算就可以用加和，然后再依据时间的推进，对于获得的奖励进行衰减，这样的一个原则来进行了。

68
--> Speaker: 教师
00:19:09,430 --> 00:19:10,760
好，那么回到咱们对于累计回报的一个定义，我们会对于从时间t开始往后形成的序列的一个奖励的累计回报给他一个表示叫 GT，那么它表示的是从t时刻开始，一直到时间的尽头所产生的奖励序列形成的累计回报，那么这个时间的尽头也就是这个轨迹它所包含的时间的步数，比如说总共如果是100步，那么这个时间的尽头就是好。

69
--> Speaker: 教师
00:19:10,770 --> 00:19:59,590
那么GT的计算公式也就变成GT等于 rt也就是t时刻当前所获得的奖励，加上伽马乘以rt加一也就是下一个时间片我们所获得的奖励。

70
--> Speaker: 教师
00:19:59,940 --> 00:20:07,440
然后因为它离这个时间片已经离我们当前有一段距离，我们会对它进行一个衰减，因为奖励越早获得越好。

71
--> Speaker: 教师
00:20:07,900 --> 00:20:31,520
那么再加上rt加2又经历了一个时间片，这个奖励又会再一次的进行衰减，乘以伽马平方加上rt加3乘以伽马三次，这样一直往下进行好了，那么基于累计回报，咱们可以去为每一个状态，他所能获得的一个期望的累计回报做一个计算。

72
--> Speaker: 教师
00:20:31,520 --> 00:20:39,460
那么期望的累计回报就表示，我如果经过这个状态，我有可能获得的累积回报是多少。

73
--> Speaker: 教师
00:20:40,080 --> 00:20:51,750
那么具体的计算方法就是咱们可以观察很多个这样的序列，然后在这些序列里面看到，我如果经过了s我能够获得的累计回报是多少？

74
--> Speaker: 教师
00:20:52,080 --> 00:21:06,680
那么我们计算很多个轨迹之后，就可以为当前这个状态计算一个期望的累积回报值，我们用vs来表示这个状态的一个期望累积回报值，也叫效用值。

75
--> Speaker: 教师
00:21:06,680 --> 00:21:36,210
那么它的计算实际上是基于GT来进行的，好，咱们还是回到火星车这样的一个那么刚才我们形成了一个轨迹，是从s4获得奖励，0到s5获得奖励，0到s6获得奖励，0到s7获得奖励证实，那么累积回报是多少？

76
--> Speaker: 教师
00:21:36,210 --> 00:22:25,610
那么这样的一个奖励序列是000证实，那么当前这个时间片是获得了0的奖励，加上咱们这个伽马衰减因子设定成是1/2，也就是0+1/2×0+1/4×0+1/8×10，最后就是等于1.25，所以现在奖励序列它包含了4个奖励，那么它的时间尽头就是4，如果说第一个时间片是一的话，我们就要考虑4个奖励所形成的累计回报，那么每一个时间片所获得的奖励都会依据它距离开始时间的这个关系来进行衰减，那么到证实的时候它已经衰减了三次，也就是10×1/8，最后我们可以获得1个累积回报值的计算。

77
--> Speaker: 教师
00:22:25,610 --> 00:22:41,340
好了，那么我们进一步的来看，这里有7个状态，我们会去衡量每一个状态，它的期望累积回报值可以通过 Vs的计算公式来进行得到。

78
--> Speaker: 教师
00:22:41,910 --> 00:22:57,260
那么我们收集了很多个跟某一个状态所对应的轨迹之后，来看这个轨迹能够获得的累计回报是多少，然后为这样的一些样本求一个期望，就可以得到每个状态所对应的期望，累积回报值了。

79
--> Speaker: 教师
00:22:57,680 --> 00:23:01,610
那么整个计算过程我们会在后面的内容里面学到。

80
--> Speaker: 教师
00:23:01,610 --> 00:23:50,100
当然了对于这个问题来讲，我们求出来 vs一它的一个期望累计回报是1.53，那么 vs7是最大的，它的期望累计回报是15.31，做完了所有的铺垫之后，咱们终于来到了马尔可夫决策过程，那么马尔可夫决策过程，它实际上就是马尔可夫过程，加上奖励，再加上动作，马可夫修斯过程里面包含了5个元素，其中第一个os就是状态空间，a是动作空间，是在马尔可夫奖励过程当中的一个添加，它指的是在某个状态下我们可以采取的动作所形成的一个空间。

81
--> Speaker: 教师
00:23:50,100 --> 00:24:11,290
那么咱们也有状态转移模型，只是这个状态转移模型会比马尔可夫奖励过程里面的状态转移模型相对复杂，因为下一个状态的取值的一个分布不仅仅依赖于当前这个状态，它也依赖于当前这个状态下我们采取了什么样的一个动作，所以这个状态转移矩阵的条件部分就有了 t时刻下的状态以及t时刻下你采取了什么样的一个动作。

82
--> Speaker: 教师
00:24:12,100 --> 00:24:40,380
那么在没有动作的情况下，我们的状态转移矩阵它的维度是n乘n的一个矩阵，那么在马尔可夫决策过程的情形下，我们的状态转移矩阵就变成了 sa乘以s也就是s平方乘以a这么多个参数。

83
--> Speaker: 教师
00:24:40,380 --> 00:24:55,020
那么同样对于这个奖励函数来说，我们也要考虑状态跟动作的一个组合，那么每一个奖励值也就跟一个状态跟一个动作的组合是相关的。

84
--> Speaker: 教师
00:24:55,020 --> 00:25:00,690
在某一个状态下采取了某个特定的动作，他所能获得的一个期望奖励是多少？

85
--> Speaker: 教师
00:25:00,700 --> 00:25:20,150
那么它的维度也就变成了s乘以a那么这个a是一个大a表示的是动作空间的元素的个数，那么伽马同样是一个0~1之间的实数，它是计算某一个奖励序列的累计回报时候用来做衰减的。

86
--> Speaker: 教师
00:25:21,700 --> 00:25:29,650
那么mdp它就包含了5个元素，这5个元素分别是s和伽马。

87
--> Speaker: 教师
00:25:29,650 --> 00:25:41,540
我们来看一下对于火星车这样的一个例子，我们如果用马可夫决策过程来对它进行建模的话，会是什么一个样子？

88
--> Speaker: 教师
00:25:43,070 --> 00:25:54,550
火星车在1个走廊式的空间里面移动，那么这个空间里面总共有7个位置，我们给每1个位置都设定为对应的1个状态，所以状态空间就是7个状态。

89
--> Speaker: 教师
00:25:55,790 --> 00:26:08,570
那么在现在的设定下，火星车它有自主移动的权利了，他可以选择向左移或者是向右移，那么这个动作空间就是向左或者是向右。

90
--> Speaker: 教师
00:26:08,570 --> 00:26:30,210
在有了状态空间跟动作空间之后，我们要考虑这个状态转移模型是什么样子的，那么状态转移模型是在t时刻下，我所处的状态选择一个动作之后，会去到下一个状态的概率分布。

91
--> Speaker: 教师
00:26:30,210 --> 00:26:44,130
那么我们该怎么画这样的一个状态转移矩阵，显然它现在不止一个矩阵，我有多少个动作就有多少个 n乘n的矩阵。

92
--> Speaker: 教师
00:26:44,390 --> 00:26:58,750
在现在这个问题下，我们有多少个动作的，可能就会有多少个7×7的矩阵，那么我们有两个动作，所以就可以画出两个7×7的矩阵，分别是向左的或者是向右的。

93
--> Speaker: 教师
00:26:58,750 --> 00:27:19,420
那么在这里为了把这个问题进行简化，我们假设这个动作的执行是确定的，不包含随机性的，所以如果说在某个状态下，我采取了向左的动作，如果在状态是s一的，它会撞墙，然后就会停留在原地。

94
--> Speaker: 教师
00:27:19,420 --> 00:27:28,970
那么如果是s二，我选择向左的动作，它就会去到s一，如果是s三采取向左的动作，它就会去到s二，以此类推。

95
--> Speaker: 教师
00:27:28,970 --> 00:27:37,400
那么同样另外一个矩阵表示着，如果在某个状态下，我采取了向右这个动作，我可以去到的状态是什么？

96
--> Speaker: 教师
00:27:37,410 --> 00:27:48,170
如果状态是s一的话，我会去到s二是s二的话会去到s三，这就是状态转移的模型，两个7×7的矩阵。

97
--> Speaker: 教师
00:27:48,170 --> 00:28:11,650
好了，那么对于马尔可夫决策过程来说，我们同样会产生一个轨迹，那么这里的轨迹就包含了三种不同的元素，它分别是状态下选择的这个动作，在某个状态下选择一个动作之后所获得的奖励，接着完成状态的转移。

98
--> Speaker: 教师
00:28:12,520 --> 00:28:21,030
所以我们看到的一个轨迹就是s4采取动作向右获得奖励0转移到s5，采取动作向右获得奖励0，转移状态到s6，采取动作向右，获得奖励是0。

99
--> Speaker: 教师
00:28:21,510 --> 00:28:36,760
同样我们如果在s4一直采取向左的动作就可以获得奖励，0去掉s3向左获得0，去掉s二向左获得零。

100
--> Speaker: 教师
00:28:37,340 --> 00:28:41,590
那么这是马尔可夫决策过程下我们所形成的轨迹。

101
--> Speaker: 教师
00:28:43,570 --> 00:28:53,970
那么同样基于这样的轨迹，我们也可以去算出每个轨迹的累计回报，它的计算过程跟马可夫奖励过程当中的累积回报的计算是。

102
--> Speaker: 教师
00:28:53,970 --> 00:29:39,240
好了，那么对于一个马可夫决策过程来说，我们想对它进行解决，它的答案是策略，那么这个策略是一个我们给定状态之下，它对于某个动作的一个概率分布，也就是说比如说在状态s一，我们的算法会推荐它去执行向左还是向右，那么最一般化的情况是我们认为它是一个概率分布，比如说我认为向左是60%，向右是40%，那么这种情况下，我们的策略它就是一个带有随机性的这样的一个概率分布。

103
--> Speaker: 教师
00:29:39,240 --> 00:29:47,810
当然了策略也可以是确定的，比如说我给定一个状态之后，我就告诉你你应该百%的执行向左或者向右。

104
--> Speaker: 教师
00:29:48,680 --> 00:30:02,460
那么在我们本次的课程当中，我们所面对的这样的一个策略都是确定性的策略，也就是给定一个状态之后，我会直接推荐给你，你应该采取什么样的动作。

105
--> Speaker: 教师
00:30:05,980 --> 00:30:19,550
那么如果说这样的一个状态，它有 n个，那么我们的策略它实际上也就是一个n维的向量，在每个位置都对应的是当前状态下应该采取的动作。

106
--> Speaker: 教师
00:30:19,550 --> 00:30:34,310
那么对于马克思决策过程来讲，我们会希望找到一个最优的策略，它告诉我们我在每个状态下，如果采取了这个动作，可以获得最大的期望累积回报，也就是让vs最大的那样的一个动作。

107
--> Speaker: 教师
00:30:34,320 --> 00:30:34,540
好。

108
--> Speaker: 教师
00:30:35,290 --> 00:30:40,640
接下来我们来思考一个很简单的问题，还是火星车这样的一个例子。

109
--> Speaker: 教师
00:30:41,540 --> 00:30:53,570
如果说有2个动作，1个向左或者是向右，那么当状态有7个的时候，对于这个问题总共有多少个可行的策略？

110
--> Speaker: 教师
00:30:53,580 --> 00:31:05,240
因为每个状态我们都有向左或者是向右两种动作的选择，所以我们会形成的这样的一个策略，就是2×2，也就是2的7次方个不同的策略。

111
--> Speaker: 教师
00:31:05,250 --> 00:31:20,760
那么每一个策略所对应的是每个状态下可以采取的动作。

112
--> Speaker: 教师
00:31:21,630 --> 00:31:28,180
刚才咱们已经介绍了马尔可夫决策过程的一个基本的设定，然后也看了火星车这样的一个例子。

113
--> Speaker: 教师
00:31:28,190 --> 00:31:58,380
好了，为了巩固大家对于马尔可夫决策过程的一个理解，咱们再多看几个例子，这个是一个赛车的例子，那么车子他希望开得更快更好，那么对于这个车子来说，我们会去观察它引擎的温度，并以此来作为一个状态，那么引擎的温度有可能是相对比较冷的，或者是已经有些热了，或者是已经过热了，那也就是所谓的爆缸了。

114
--> Speaker: 教师
00:31:58,380 --> 00:32:04,340
那么在这儿爆缸这样的一个状态也是一个结束状态，如果引擎爆缸了，那么游戏就结束了。

115
--> Speaker: 教师
00:32:06,400 --> 00:32:10,060
那么对于这个车子它有两个动作可以采取，一个是减速，一个是加速。

116
--> Speaker: 教师
00:32:10,070 --> 00:32:30,310
那么这个问题里面咱们可以观察到，不同的引擎状态之间，它通过采取不同的动作是可以有一些转移的，然后同时我采取了不同的状态下，采取不同的动作，可以获得不一样的奖励值。

117
--> Speaker: 教师
00:32:30,310 --> 00:32:46,380
它的状态转移的情况可以从这个图里面看到，比如说当引擎的状态是冷的时候，你采取减速，那么有百%的情况你会回到引擎是冷的状态下，并且获得加一的奖励。

118
--> Speaker: 教师
00:32:46,380 --> 00:33:03,790
那么如果引擎是冷的情况下，你采取了加速，那么有50%的情况你还是维持在冷的状态下，并且获得正二的奖励，那么有剩余50%的可能你会去到引擎变热的情况，获得的奖励还是正二。

119
--> Speaker: 教师
00:33:03,790 --> 00:33:22,790
那么如果说这个车子的引擎现在是热的情况下，你采取了减速，那么有50%你的引擎会变成冷的状态，然后获得正一的奖励，有另外50%你还是维持在热的状态下，并且获得正一的奖励。

120
--> Speaker: 教师
00:33:22,790 --> 00:33:30,560
那么如果在引擎是热的情况下，你采取了加速这个动作，会百%的爆缸，那么游戏就结束了。

121
--> Speaker: 教师
00:33:31,320 --> 00:33:39,580
好，这个是我们面对的一个最基本的场景，咱们想把它建模成一个马尔可夫决策的过程。

122
--> Speaker: 教师
00:33:39,580 --> 00:34:16,130
大家应该还记得马尔可夫决策过程它有5大基本的要素，分别是状态空间、动作空间、转移模型、奖励模型，还有衰减因子，那么衰减因子是我们可以作为先验的一个参数来设定的，那么我们就先放在这儿，假设它已经有了状态空间，它有三个状态，分别是引擎冷热以及爆缸，那么动作有两个减速或者是加速，那么状态转移模型会是什么？

123
--> Speaker: 教师
00:34:16,130 --> 00:34:42,050
我们刚提到了，对于马可夫决策过程来讲，我们会有大a个n乘n的矩阵，大a表示的是动作的个数，那么在现在这个情况下，咱们就有两个3×3的矩阵，每一个矩阵它对应的都是一个动作，比如说在减速的情况下，我们的状态转移是怎么样的，加速情况下我们的状态转移是怎么样的？

124
--> Speaker: 教师
00:34:42,060 --> 00:35:09,410
所以当减速的情况下，咱们引擎是冷的，是百%去到了冷的，那么如果是热的情况下，会有50%去到引擎变冷，50%维持在引擎是热的，那么对于动作加速，我如果是状态在冷的情况下，会有50%维持在冷的状态下，50%去掉引擎变热的情况。

125
--> Speaker: 教师
00:35:09,410 --> 00:35:28,650
那么对于状态是热的情况下，如果加速会有百%去到爆缸这样的一个那么咱们的奖励模型里面，它的入口就有 s乘以a个，那么也就是状态是3个，动作是2个，也就是6个数值。

126
--> Speaker: 教师
00:35:28,650 --> 00:35:38,670
那么从这个模型里面我们也可以观察到，在状态是冷的情况下，减速获得正一的奖励，状态是冷的情况下，加速获得是正二的奖励。

127
--> Speaker: 教师
00:35:38,670 --> 00:36:28,350
状态是热的情况下减速获得正一，状态是热的情况下，加速获得是-10，因为直接就爆缸，然后游戏结束，那么这样我们就把一个赛车的问题建模成了一个马尔可夫决策的过程，包含了5大元素，状态、动作空间、状态转移模型以及奖励模型，还有一个预先设定的先验的参数衰减因子，这是第一个例子，然后我们回头再看一下咱们课程最开始讲的格子世界的例子，我们同样用五大要素来把格子世界建模成是一个马尔可夫决策过程。

128
--> Speaker: 教师
00:36:28,360 --> 00:36:36,240
那么在这儿我们的状态空间，因为每个格子都是一个状态，这是一个3×4的矩阵，所以我们有12个状态。

129
--> Speaker: 教师
00:36:37,060 --> 00:36:49,900
当然了这里面的2×2这样的一个状态是不可达的，因为它是一一堵墙，那么这里4二这个状态它是一个结束状态，会获得的奖励会是-1。

130
--> Speaker: 教师
00:36:50,180 --> 00:36:59,390
那么43也是一个结束状态，能获得的奖励是正一，那么一一是我们的开始状态，接着是第二个元素，也就是动作空间。

131
--> Speaker: 教师
00:36:59,780 --> 00:37:04,590
在格子世界里，智能体可以采取向上向下向左向右4个动作。

132
--> Speaker: 教师
00:37:04,590 --> 00:37:36,730
那么第三个元素是状态转移模型，状态转移模型有大a个n乘n的矩阵，那么有4个动作，所以我们有4个12×12的矩阵，那么这个状态转移矩阵就会描绘出我在动作执行的过程当中所伴随着的随机性，比如说有1080%的情况是可以朝着我想要去执行的动作方向去的，10%会平分在垂直于我想去的那两个方向。

133
--> Speaker: 教师
00:37:36,730 --> 00:37:56,030
那么奖励模型它是跟1个状态和动作的组合相关的，那么这儿有12个状态，有4个动作，所以我们有48个不同的奖励值，那么衰减因子是我们预先设定的，那么这就有了格子世界的马尔可夫决策过程的一个基本设定。

134
--> Speaker: 教师
00:37:56,750 --> 00:37:58,490
那么里面状态转移矩阵长什么样？

135
--> Speaker: 教师
00:37:58,830 --> 00:38:00,240
奖励模型长什么样？

136
--> Speaker: 教师
00:38:00,610 --> 00:38:02,400
大家可以试一试。

137
--> Speaker: 教师
00:38:04,730 --> 00:38:23,130
好，那么接下来我们会进一步的去关注一下，因为如之前所说奖励模型也是跟问题共同存在的，环境会给什么样的奖励，一般是我们碰到一个问题之后就会对奖励值进行设定。

138
--> Speaker: 教师
00:38:23,130 --> 00:38:42,990
比如说汽车的加速或减速，它所对应的奖励值我可以给加速证实减速，正一也可以给加速正20减速正15等等不同的设定，它会使得我们的智能体朝着不同的方向去选择自己的策略。

139
--> Speaker: 教师
00:38:42,990 --> 00:38:47,250
那么奖励跟策略之间它的关系是怎么样的？

140
--> Speaker: 教师
00:38:47,260 --> 00:38:51,040
咱们来以格子世界为例看一看。

141
--> Speaker: 教师
00:38:51,730 --> 00:38:54,120
那么格子世界里面它有两种类型的奖励，第一种是游戏结束时候的奖励，咱们先设定不变，也就是在4~2的时候结束游戏会-1，那么在4~3结束游戏的时候会获得正一的奖励。

142
--> Speaker: 教师
00:38:54,130 --> 00:39:13,870
那么另外还有一个及时奖励，也就是单步的代价，我每多走一步需要付出的代价是多少？

143
--> Speaker: 教师
00:39:13,880 --> 00:39:32,840
如果说我们设定这个代价是-0.01，那么通过算法咱们可以给不同的状态找到它所对应的最佳动作，也就是这个图里面所表示的，那么咱们关注4个不同的状态，第一个状态是一一也就是开始的状态，那么开始的状态它实际上有两条路径可以去往游戏结束的状态。

144
--> Speaker: 教师
00:39:34,010 --> 00:39:54,320
那么当然了，我们的智能体它是一个向上的智能体智能体会，希望获得的累积奖励是越多越好，所以目标的状态一般都是43。

145
--> Speaker: 教师
00:39:55,600 --> 00:40:08,280
那么对于一这个位置上智能体就可以采取向上再向右这样的一条路径去到正一，或者是向右再向上这个路径去到43的位置。

146
--> Speaker: 教师
00:40:08,280 --> 00:40:30,550
那么第二个我们想要去观察的状态是31，这里刚好是一个分岔路口，那么咱们的智能体可以选择向上再向右直接去到正一，或者是绕道经过一一格子再去到正一，那么我们会观察不同的奖励设定对于它的影响是什么。

147
--> Speaker: 教师
00:40:30,550 --> 00:40:44,410
另外我们还会观察两个状态，这两个状态是跟-1这样的一个不好的结束状态所相邻的两个，一个是41，还有一个是32。

148
--> Speaker: 教师
00:40:44,410 --> 00:40:55,460
好，那么咱们首先来看，如果这个环境下我们设定的单步的代价是-0.01的话，这4个状态下智能体会采取什么样的动作？

149
--> Speaker: 教师
00:40:55,470 --> 00:40:57,060
在一这个位置上呢？

150
--> Speaker: 教师
00:40:57,320 --> 00:41:21,860
智能体选择了向上走，那么显然了，因为我们可以算一算，如果说每一步都百%执行到位的话，那么智能体它会在先经过122333，然后再去到435步可以到达我们的结束的位置。

151
--> Speaker: 教师
00:41:21,860 --> 00:41:52,030
那么如果它选择向右这条路径的话，它会经过2131、3233再去到那么这两条路径经过的步数是一样的，但差别是如果我选择了走右边这条路去正一，那么经过32这个位置的时候，就有一定的概率会向右走去到-1这个位置，然后付出一些代价。

152
--> Speaker: 教师
00:41:52,320 --> 00:41:59,560
所以为了避免这样的一个风险，智能体会选择向上走，这个动作是符合我们的想象的。

153
--> Speaker: 教师
00:42:00,510 --> 00:42:07,340
那么对于分岔路口31的这个位置，我们的智能体选择了绕道走，而不是走近路直接去到正一。

154
--> Speaker: 教师
00:42:08,170 --> 00:42:26,880
那么这里的原因是因为当前我们绕道所付出的代价，它要小于我经过32格子，然后被卷到-1这个位置所要付出的风险，那么所以智能体在31这个位置代价小的时候，他会选择绕道。

155
--> Speaker: 教师
00:42:27,590 --> 00:42:39,760
那么我们看41跟32这两个跟-1相邻的格子智能体，它不选择直接去到正一的这个位置，而是选择远离负一的这个位置。

156
--> Speaker: 教师
00:42:40,350 --> 00:43:26,480
那么这里的原因是如果说智能体它选择更快的去往正一的路径，都会有一定的概率会卷入到-1这样的一个不好的结束状态里，那么这个代价是很大的，所以他们宁愿选择撞墙，那么有一定的概率可以去到离正义更近的那样的一个位置，那么这就是在当我们单步代价比较小的时候，有很多的时间可以去消耗，所以我们会倾向于去选择用更复杂的方法来逃避，用负一结束游戏这个结果。

157
--> Speaker: 教师
00:43:26,480 --> 00:43:37,360
好了，接下来我们来看第二种情形，当单步代价它变成了-0.03，也就是代价变大的时候，这4个不同的状态下，智能体会选择的动作的一个变化。

158
--> Speaker: 教师
00:43:37,850 --> 00:44:24,960
那么在一这个位置智能体依旧选择向上走，然后去往正义，那么在31这个位置智能体的选择也还是一样的，但是在跟-1相邻的这两个状态下，41还有32智能体不撞墙了，因为这个时候他在等待着10%的概率把他送往离正一更近的位置，他需要等待的时间太长了，那么它所给出来的累计回报已经超过了，它被卷入到-1格子的风险了，所以在c这个位置，他会选择直接向左走，那么在32这个位置，他会选择直接向上走，去冒10%的被卷入到负1结束游戏的这样的一个风险。

159
--> Speaker: 教师
00:44:24,960 --> 00:45:09,120
那么当我们的单步的代价在继续提升的时候，每走一步我需要付出-0.4的代价的时候，我们再来看这4个状态的变化，一一依旧选择向上这样的一条路径去走，那么在31的位置，我们的智能体选择不绕道了，我要走最直接的路径去往正一，因为当单步的代价是-0.4的时候，我只要走三步，我所付出的代价值就已经超过了我卷入到负一结束游戏所付出的代价，所以智能体不选择绕道了。

160
--> Speaker: 教师
00:45:09,120 --> 00:45:19,450
那么在41的这个位置跟32的位置智能体还是会选择用最直接的方式去往正一的路径。

161
--> Speaker: 教师
00:45:19,450 --> 00:45:27,360
好，咱们再来看一个更极端的情况，当我的单步代价变成-2.1的时候，这表示什么意思呢？

162
--> Speaker: 教师
00:45:28,330 --> 00:45:36,830
这表示我每走一步所付出的代价都已经要比被卷入到负一结束游戏付出的代价要更大了。

163
--> Speaker: 教师
00:45:37,790 --> 00:45:44,420
也就是说只要游戏每进行一刻，我都要付出更惨重的代价，所谓的生不如死。

164
--> Speaker: 教师
00:45:44,430 --> 00:45:50,700
那么在这种情况下，对于智能体它最好的一个动作选择就是尽快的结束游戏。

165
--> Speaker: 教师
00:45:50,700 --> 00:45:56,060
好了，那么尽快的结束游戏，我们有两个结束状态，一个是正1，一个是-1。

166
--> Speaker: 教师
00:45:56,070 --> 00:46:03,160
那么目前对于智能体来讲，不管去到哪个结束状态都是更好的一种选择。

167
--> Speaker: 教师
00:46:03,750 --> 00:46:22,640
所以好，那么在一一这样的一个状态下，它的目标就是离得更近的结束状态，而不是正一这样的一个结束状态，所以他会选择向右这样的一条路径，因为他只要走4步就可以直接到-1结束状态了。

168
--> Speaker: 教师
00:46:23,330 --> 00:46:27,520
而如果向上走的话，他需要走5步才能去到正一这个位置。

169
--> Speaker: 教师
00:46:27,530 --> 00:46:58,710
那么在分岔路口的31的点，他会选择向右，因为会有更大的概率去往-1这个结束的状态，那么我们离-1这个状态更接近的这两个状态，他都会选择我直接去往-1这个状态来结束游戏好了，通过我们去修改单步奖励的值，会对智能体的一个策略选择发生很大的一个影响。

170
--> Speaker: 教师
00:46:59,650 --> 00:47:01,270
那么这就是这个例子告诉我们。

